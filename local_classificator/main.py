import pandas as pd
import json
import os
import requests
from typing import List, Dict, Optional
from pathlib import Path

from llama_cpp import Llama


def download_model(url: str, model_path: str) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â—ë –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    
    Args:
        url: URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        model_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        
    Returns:
        bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Å–∫–∞—á–∞–Ω–∞ –∏–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    """
    if os.path.exists(model_path):
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {model_path}")
        return True
    
    print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —Å {url}...")
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(model_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        print(f"\rüì• –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%", end="", flush=True)
        
        print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞: {model_path}")
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        if os.path.exists(model_path):
            os.remove(model_path)  # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
        return False


class LLMLocal:
    def __init__(
        self,
        model_path: str,
        n_ctx: int = 4096,
        n_threads: int = 4,
        n_gpu_layers: int = 35,
        verbose: bool = False,
        use_system_role: bool = True
    ):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")

        print(f"üîß –ó–∞–≥—Ä—É–∂–∞–µ–º GGUF –º–æ–¥–µ–ª—å: {model_path}")
        self.llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_gpu_layers=n_gpu_layers,
            verbose=verbose,
        )
        self.use_system_role = use_system_role

    def answer(
        self,
        messages: list,
        max_new_tokens: int = 256,
        temperature: float = 0.1
    ) -> str:
        try:
            if self.use_system_role:
                output = self.llm.create_chat_completion(
                    messages=messages,
                    max_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=0.9,
                )
                return output["choices"][0]["message"]["content"].strip()
            else:
                # –°–∫–ª–µ–∏–≤–∞–µ–º system + user –≤ –æ–¥–∏–Ω user-–∑–∞–ø—Ä–æ—Å
                full_content = ""
                for msg in messages:
                    if msg["role"] == "system":
                        full_content += f"{msg['content']}\n\n"
                    elif msg["role"] == "user":
                        full_content += f"–û—Ç–∑—ã–≤:\n{msg['content']}"
                output = self.llm.create_chat_completion(
                    messages=[{"role": "user", "content": full_content}],
                    max_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=0.9,
                )
                return output["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return ""



class LLMClassifier:
    def __init__(
        self,
        llm,
        categories: List[str],
        system_prompt: Optional[str] = None
    ):
        self.llm = llm
        self.categories = [cat.strip() for cat in categories]
        self.default_system_prompt = system_prompt or self._default_prompt()

    def _default_prompt(self) -> str:
        cats = ", ".join([f'"{cat}"' for cat in self.categories])
        return f"""–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –æ—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞.
–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {cats}.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–∑—ã–≤ –∏ –≤—ã–¥–µ–ª–∏ –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
{{
  "annotations": [
    {{
      "category": "...",
      "summary": "...",
      "sentiment": "–ø–æ–∑–∏—Ç–∏–≤|–Ω–µ–≥–∞—Ç–∏–≤|–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    }}
  ]
}}
–ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π."""

    def classify(
        self,
        text: str,
        system_prompt: Optional[str] = None,
        max_new_tokens: int = 300,
        temperature: float = 0.1
    ) -> dict:
        prompt = system_prompt or self.default_system_prompt

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text}
        ]

        raw_response = self.llm.answer(
            messages=messages,
            max_new_tokens=max_new_tokens,
            temperature=temperature
        )

        return self._clean_json(raw_response)

    def _clean_json(self, text: str) -> dict:
        try:
            start = text.find("{")
            end = text.rfind("}") + 1
            if start == -1 or end == 0:
                return {"annotations": []}
            cleaned = text[start:end]
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e} | –¢–µ–∫—Å—Ç: {text[:300]}...")
            return {"annotations": []}


import random
# –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def classify_test(results: List[dict], allowed_categories: List[str], n_samples: int = 3):
    allowed_set = set(cat.strip() for cat in allowed_categories)
    found_categories = set()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for item in results:
        for ann in item.get("annotations", []):
            cat = ann.get("category", "").strip()
            found_categories.add(cat)
            if cat not in allowed_set:
                print(f"üî¥ ID {item['id']} ‚Äî wrong category: '{cat}'")

    print(f"\nüìä –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {sorted(found_categories)}")
    status = "‚úÖ –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã." if all(c in allowed_set for c in found_categories) else "‚ùå –ï—Å—Ç—å –Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ."
    print(status)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    cat_to_reviews = {cat: [] for cat in allowed_set}
    for item in results:
        for ann in item.get("annotations", []):
            cat = ann.get("category", "").strip()
            if cat in cat_to_reviews:
                cat_to_reviews[cat].append(item)

    # –ü—Ä–∏–º–µ—Ä—ã
    print(f"\nüéØ –ü—Ä–∏–º–µ—Ä—ã (–ø–æ {n_samples} –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é):")
    for cat in sorted(cat_to_reviews.keys()):
        reviews = cat_to_reviews[cat]
        if not reviews:
            continue
        print(f"\nüìå '{cat}' ({len(reviews)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)")
        sampled = random.sample(reviews, min(n_samples, len(reviews)))
        for r in sampled:
            summaries = [a.get("summary", "...") for a in r.get("annotations", []) if a.get("category") == cat]
            summary_text = "; ".join(summaries[:2])
            print(f"  üîπ ID {r['id']} ‚Äî {summary_text[:150]}...")


def csv_read(filepath: str, nrows: Optional[int] = None) -> pd.DataFrame:
    df = pd.read_csv(filepath, nrows=nrows)
    required = ["id", "text"]
    if not all(col in df.columns for col in required):
        raise ValueError(f"CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: {required}")
    return df

def save_checkpoint(data: List[dict], filename: str):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")


# === –ù–ê–°–¢–†–û–ô–ö–ò ===
# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—É–∂–µ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω)
categories_list = [
     "–ö–∞—Ä—Ç—ã",
    "–ë–∞–Ω–∫–æ–º–∞—Ç—ã",
    "–ö—ç—à–±—ç–∫ / –ë–æ–Ω—É—Å—ã",
    "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ñ–∏—Å–µ",
    "–í–∫–ª–∞–¥—ã",
    "–ö—Ä–µ–¥–∏—Ç—ã",
    "–ö—É—Ä—å–µ—Ä—Å–∫–∞—è —Å–ª—É–∂–±–∞",
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ / —Å–∞–π—Ç",
    "–°–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏",
    "–°—á–µ—Ç–∞",
    "–ü—Ä–æ—á–∏–µ —É—Å–ª—É–≥–∏"
]

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM_PROMPT = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç–∑—ã–≤–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–∑—ã–≤ –∫–ª–∏–µ–Ω—Ç–∞ –±–∞–Ω–∫–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏–ª–∏ —É—Å–ª—É–≥–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è, –∞ —Ç–∞–∫–∂–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–∞ (–ø–æ–∑–∏—Ç–∏–≤/–Ω–µ–≥–∞—Ç–∏–≤/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π)

–ü—Ä–∞–≤–∏–ª–∞:
1. –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¢–û–õ–¨–ö–û –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ø–∏—Å–∫–∞: {categories_list} . –ó–∞–ø—Ä–µ—â–µ–Ω–æ –≤—ã–¥—É–º—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ.
2. –ö–∞—Ç–µ–≥–æ—Ä–∏—è "–ü—Ä–æ—á–∏–µ —É—Å–ª—É–≥–∏" –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –∏–∑ –ø—Ä–æ—á–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç.
3. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–æ –∏–∑ —Ç—Ä–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π: –ø–æ–∑–∏—Ç–∏–≤, –Ω–µ–≥–∞—Ç–∏–≤ –∏–ª–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π.
   –ï—Å–ª–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –æ—Ç–∑—ã–≤–∞ —Ä–∞–∑–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ - –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ –Ω–µ–≥–∞—Ç–∏–≤.
   –ù–∞–ø—Ä–∏–º–µ—Ä "–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –æ—á–µ–Ω—å –±–æ–≥–∞—Ç—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, –Ω–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–æ–ª–≥–æ –≥—Ä—É–∑–∏—Ç—Å—è" = –Ω–µ–≥–∞—Ç–∏–≤.
4. –û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –ù–µ –¥–æ–±–∞–≤–ª—è–π —ç–ª–µ–º–µ–Ω—Ç—ã markdown, –Ω–µ –æ—Ç–≤–µ—á–∞–π –ø—Ä–æ—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:

  {{ "annotations": [{{"category": "...", "summary": "...", "sentiment": "..." }}, ...] }}

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON.
"""

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ GGUF –∏ URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
MODEL_PATH = "model.gguf"
MODEL_URL = "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf"

# –ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ—Å—Ç–∞–≤—å None –¥–ª—è –≤—Å–µ—Ö)
TEST_ROWS = None

# –î–ª—è classify_test - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
N_SAMPLES_PER_CATEGORY = 3

# –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤
CHECKPOINT_EVERY = 5
OUTPUT_JSON = "llm_results.json"

# === 1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) ===
if not download_model(MODEL_URL, MODEL_PATH):
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
    exit(1)

# === 2. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ===
df = csv_read("total_data_banki_i_sravni.csv", nrows=TEST_ROWS)
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")

# === 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ===
try:
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        n_gpu_layers = 35 if gpu_available else 0
        print(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpu_available}")
    except ImportError:
        n_gpu_layers = 0
        print("üñ•Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    
    llm_engine = LLMLocal(
        model_path=MODEL_PATH,
        n_gpu_layers=n_gpu_layers,
        use_system_role=False      # ‚ö†Ô∏è –í–∞–∂–Ω–æ: —ç—Ç–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç system role
    )
    classifier = LLMClassifier(llm_engine, categories_list, SYSTEM_PROMPT)
    print("‚úÖ LLM –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
except Exception as e:
    print(f"üî¥ –û—à–∏–±–∫–∞: {e}")
    raise

# === 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ ===
results = []

for idx, row in df.iterrows():
    review_id = row["id"]
    text = str(row["text"]).strip()

    print(f"[{idx+1}/{len(df)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–∞ {review_id}...")
    annotation = classifier.classify(text)

    results.append({
        "id": review_id,
        "annotations": annotation.get("annotations", [])
    })

    # –ß–µ–∫–ø–æ–∏–Ω—Ç
    if (idx + 1) % CHECKPOINT_EVERY == 0 or (idx + 1) == len(df):
        save_checkpoint(results, OUTPUT_JSON)

print(f"\nüéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} –æ—Ç–∑—ã–≤–æ–≤")


# === –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ ===
classify_test(results, categories_list, N_SAMPLES_PER_CATEGORY)

# === 5. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
save_checkpoint(results, OUTPUT_JSON)
print(f"üì§ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_JSON}")
